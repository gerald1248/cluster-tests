apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ .Values.name | quote }}
  namespace: {{ .Values.namespace | quote }}
data:
  get_context: |
    kubectl config current-context
  get_nodes: kubectl get nodes -o jsonpath="{.items[*].metadata.name}"
  get_user_namespaces: |
    kubectl get ns -o jsonpath="{.items[*].metadata.name}" | tr " " "\n" | grep -v "\(kube-public\|kube-system\)"
  ignore: |
    test_identity
  test_services_ha: |
    for svc in ${HA_SERVICES}; do
      nodes=`kubectl get po --all-namespaces -o wide | grep ${svc} | \
        awk '{ print $8 }'`
      zones=""
      for node in ${nodes}; do
        zones="${zones} `kubectl get node/${node} -L "failure-domain.beta.kubernetes.io/zone" | \
          awk '{print $6}' | \
          tail -n +2`"
      done
      zone_count=`echo ${zones} | tr ' ' '\n' | sort -u | wc -l`
      ha=false
      if [ "${zone_count}" -lt "2" ]; then
        echo "${svc} must be distributed across two or more zones"
        exit 1
      fi
    done
  test_pods_healthcheck: |
    for namespace in ${USER_NAMESPACES}; do
      for pod in `kubectl get pods --field-selector=status.phase=Running -n ${namespace} -o jsonpath="{.items[*].metadata.name}"`; do
        result=`kubectl get pod/${pod} -n ${namespace} -o json | jq '.spec.containers[] | select(.readinessProbe and .livenessProbe)'`
        if [ "${#result}" -eq "0" ]; then
          echo "pod ${pod} in namespace ${namespace} requires both liveness and readiness probes"
          exit 1
        fi
      done
    done
  test_cluster_admin_bindings: |
    sa_with_cluster_admin=`kubectl get clusterrolebindings -o wide | grep "cluster-admin" | awk '{ print $5 }' | grep "[[:alnum:]]" | sed 's/^/"/' | sed 's/$/"/'`
    trimmed=`echo "${sa_with_cluster_admin}" | tr -d '[[:space:]]'`
    if [ ! -z "${trimmed}" ]; then
      count=`echo ${sa_with_cluster_admin} | wc -l`
      if [ "${count}" -ne "0" ]; then
        fail "there must be no service accounts with cluster admin rights, found ${count}: [ ${sa_with_cluster_admin} ]"
      fi
    fi
  test_identity: |-
    if [ "0" -ne "0" ]; then
      fail "expected identity"
    fi
  test_pods_escalation: |-
    for namespace in ${USER_NAMESPACES}; do
      for pod in `kubectl get pods -o jsonpath="{.items[*].metadata.name}"`; do
        count=`kubectl get po -n ${namespace} -o json | jq '.items[].spec.containers[] | select(.securityContext != "{}" and .securityContext != null and .securityContext.allowPrivilegeEscalation == true)' | wc -l`
        if [ "${count}" -ne "0" ]; then
          echo "pod ${pod} in namespace ${namespace} allows privilege escalation"
          exit 1
        fi
      done
    done
  test_nodes_warnings: |
    for node in ${NODES}; do
      warnings=`kubectl describe node/${node} | grep -A64 "^Events:" | grep -c "Warn"`
      if [ "${warnings}" -ne "0" ]; then
        fail "node ${node} has warnings"
      fi
    done
  test_nodes_ready: |-
    for node in ${NODES}; do
      result=`kubectl get node ${node} -o jsonpath="{.status.conditions[?(@.type=='Ready')].status}"`
      if [ "${result}" != "True" ]; then
        fail "node ${node} not ready"
      fi
    done
  test_nodes_out_of_disk: |-
    for node in ${NODES}; do
      result=`kubectl get node ${node} -o jsonpath="{.status.conditions[?(@.type=='OutOfDisk')].status}"`
      if [ "${result}" != "False" ]; then
        fail "node ${node} is short of disk space"
      fi
    done
  test_nodes_memory_pressure: |-
    for node in ${NODES}; do
      result=`kubectl get node ${node} -o jsonpath="{.status.conditions[?(@.type=='MemoryPressure')].status}"`
      if [ "${result}" != "False" ]; then
        fail "node ${node} is under memory pressure"
      fi
    done
  test_nodes_disk_pressure: |-
    for node in ${NODES}; do
      result=`kubectl get node ${node} -o jsonpath="{.status.conditions[?(@.type=='DiskPressure')].status}"`
      if [ "${result}" != "False" ]; then
        fail "node ${node} is under disk pressure"
      fi
    done
  test_nodes_pid_pressure: |-
    for node in ${NODES}; do
      result=`kubectl get node ${node} -o jsonpath="{.status.conditions[?(@.type=='PIDPressure')].status}"`
      if [ "${result}" != "False" ]; then
        fail "node ${node} is under PID pressure"
      fi
    done
  test_pods_privileged: |
    for namespace in ${USER_NAMESPACES}; do
      for pod in `kubectl get pods --field-selector=status.phase=Running -n ${namespace} | cut -d' ' -f1 | tail -n +2`; do
        count_privileged=`kubectl get po/${pod} -n ${namespace} -o json | jq -r '..|.securityContext?.privileged' | grep -c true`
        if [ "${count_privileged}" -ne "0" ]; then
          fail "pod ${pod} in namespace ${namespace} runs with privileged security context"
        fi
      done
    done
  test_quotas: |
    for namespace in ${USER_NAMESPACES}; do
      resourcequota=`kubectl get resourcequota -n ${namespace} | wc -l`
      if [ "${resourcequota}" -eq "0" ]; then
        fail "resourcequota not set in namespace ${namespace}"
      fi
    done
  test_pods_resources: |
    for namespace in ${USER_NAMESPACES}; do
      for deployment in `kubectl get deployments -n ${namespace} -o json | jq -r '.items[].metadata.name'`; do
        resources_count=`kubectl get deployment/${deployment} -n ${namespace} -o json | jq -r '.spec.template.spec.containers[].resources | select(.limits.cpu and .limits.memory and .requests.cpu and .requests.memory)' | grep -c "[[:alnum:]]"`
        if [ "${resources_count}" -eq "0" ]; then
          fail "missing resource limits/requests in deployment ${deployment} in namespace ${namespace}"
        fi
      done
    done
  test_pods_root: |
    for namespace in ${USER_NAMESPACES}; do
      for pod in `kubectl get pods -o jsonpath="{.items[*].metadata.name}"`; do
        uid=`kubectl exec -n ${namespace} ${pod} -- id -u`
        if [ "${uid}" -eq "0" ]; then
          fail "pod ${pod} in namespace ${namespace} runs as root"
        fi
      done
    done
  test_storage_encrypted: |-
    STORAGECLASSES=$(kubectl get pv -o json | jq -r '.items[].spec.storageClassName')
    for STORAGECLASS in ${STORAGECLASSES}; do
      ENCRYPTED=$(kubectl get sc/${STORAGECLASS} -o json | jq -r '.parameters.encrypted')
      if [ "${ENCRYPTED}" == "true" ]; then
        fail "storageclass ${STORAGECLASS} is unencrypted"
      fi
    done
  test_kubelet_version: |
    begins_with() {
      case $2 in "$1"*) true;; *) false;; esac
    }

    SERVER_MAJOR=`kubectl version -o json | jq -r .serverVersion.major`
    SERVER_MINOR=`kubectl version -o json | jq -r .serverVersion.minor`
    KUBELET_VERSION=`kubectl get nodes -o jsonpath="{.items[0].status.nodeInfo.kubeletVersion}" | sort -u`

    MATCH_STRING="v{$SERVER_MAJOR}.${SERVER_MINOR}"

    if not begins_with "${KUBELET_VERSION}" "${MATCH_STRING}"; then
      fail "server and kubelet versions must match; server version: ${KUBELET_VERSION} doesn't start with ${MATCH_STRING}"
    fi
  test_pod_ready: |
    for namespace in ${USER_NAMESPACES}; do
      for pod in `kubectl get pods -o jsonpath="{.items[*].metadata.name}"`; do
        ready=`kubectl get pod/${pod} -o jsonpath="{.status.containerStatuses[0].ready}"`
        if [ "${ready}" != "true" ]; then
          fail "pod ${pod} in namespace ${namespace} is not ready"
        fi
      done
    done
  test_pod_restarts: |
    CUTOFF=16
    for namespace in ${USER_NAMESPACES}; do
      for pod in `kubectl get pods -o jsonpath="{.items[*].metadata.name}"`; do
        restartCount=`kubectl get pod/${pod} -o jsonpath="{.status.containerStatuses[0].restartCount}"`
        if [ "${restartCount}" -gt "${CUTOFF}" ]; then
          fail "pod ${pod} in namespace ${namespace} has restarted ${restartCount} times"
        fi
      done
    done
